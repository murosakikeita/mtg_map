from __future__ import annotations
import os
from pathlib import Path
from faster_whisper import WhisperModel
from openai import OpenAI
from dotenv import load_dotenv

# ==========================================
# âœ… ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
# ==========================================
load_dotenv()

# Secrets ã¾ãŸã¯ .env ã‹ã‚‰ã‚­ãƒ¼ã‚’å–å¾—
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5-mini")

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloud ã® Secrets ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
client = OpenAI(api_key=OPENAI_API_KEY)


# ==========================================
# ðŸŽ™ Whisper ã«ã‚ˆã‚‹éŸ³å£°æ–‡å­—èµ·ã“ã—
# ==========================================
def transcribe_audio(audio_path: Path) -> str:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿”ã™"""
    model = WhisperModel("medium", compute_type="int8")
    segments, _ = model.transcribe(str(audio_path), language="ja")

    text = "".join([seg.text for seg in segments])

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆä»»æ„ï¼‰
    txt_path = audio_path.with_suffix(".txt")
    txt_path.write_text(text, encoding="utf-8")

    return text


# ==========================================
# ðŸ§  GPTã«ã‚ˆã‚‹è­°äº‹éŒ²è¦ç´„
# ==========================================
def summarize_with_llm(text: str, prompt_key: str = "default") -> str:
    """ChatGPT (OpenAI API) ã§è‡ªç„¶ãªè­°äº‹éŒ²ã‚’ç”Ÿæˆ"""
    prompts = {
        "default": (
            "ã‚ãªãŸã¯æ—¥æœ¬èªžã®è­°äº‹éŒ²ä½œæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "ä»¥ä¸‹ã®ä¼šè­°å†…å®¹ã‚’ã‚‚ã¨ã«ã€è‡ªç„¶ã§èª­ã¿ã‚„ã™ãè¦ç‚¹ã‚’ã¾ã¨ã‚ãŸè­°äº‹éŒ²ã‚’Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            "ã€Žä¼šè­°åã€ã€Žæ—¥æ™‚ã€ã€Žå‚åŠ è€…ã€ã€Žæ±ºå®šäº‹é …ã€ã€ŽToDoã€ã€Žè­°è«–ã‚µãƒžãƒªã€ã®æ§‹æˆã§ãŠé¡˜ã„ã—ã¾ã™ã€‚"
        ),
        "decision_focus": (
            "ä»¥ä¸‹ã®å†…å®¹ã‹ã‚‰ã€æ±ºå®šäº‹é …ã¨ãã®æ ¹æ‹ ãƒ»å½±éŸ¿ãƒ»æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¸­å¿ƒã«Markdownã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
        ),
        "todo_focus": (
            "ä»¥ä¸‹ã®å†…å®¹ã‹ã‚‰ã€æ‹…å½“è€…ãƒ»æœŸé™ãƒ»å†…å®¹ã«æ³¨ç›®ã—ãŸToDoãƒªã‚¹ãƒˆå½¢å¼ã§Markdownã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
        ),
    }

    system_prompt = prompts.get(prompt_key, prompts["default"])

    response = client.responses.create(
        model=OPENAI_MODEL,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text},
        ],
    )

    summary = response.output_text
    return summary


# ==========================================
# âš™ï¸ ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆéŸ³å£°â†’æ–‡å­—èµ·ã“ã—â†’è¦ç´„â†’ä¿å­˜ï¼‰
# ==========================================
def process_audio(audio_file: Path, prompt_key: str = "default") -> dict:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã€æ–‡å­—èµ·ã“ã—ï¼‹è¦ç´„çµæžœã‚’è¿”ã™"""
    text = transcribe_audio(audio_file)
    summary = summarize_with_llm(text, prompt_key)

    # ä¿å­˜å…ˆã‚’ä½œæˆ
    output_dir = Path("data/outputs")
    output_dir.mkdir(parents=True, exist_ok=True)

    md_path = output_dir / f"{audio_file.stem}.minutes.md"
    md_path.write_text(summary, encoding="utf-8")

    return {
        "transcript": text[:800] + "..." if len(text) > 800 else text,
        "summary": summary,
        "md_path": str(md_path),
    }
